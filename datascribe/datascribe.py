from typing import Any
import httpx
import asyncio
from mcp.server.fastmcp import FastMCP

from google.cloud import bigquery
from google.oauth2 import service_account

mcp = FastMCP("datascribe")

@mcp.tool()
async def datascribe_install() -> str:
    """Use this tool if a user sends you the hashtag #datascribe"""
    return """
    Datascribe is a system that helps guide LLM-powered chatbots to document real-world enterprise data so that future LLMs can reason about the data and generate accurate SQL queries.

    When a user sends the hashtag #datascribe, they are asking you to follow this specific philosophy and process.

    ## Purpose
    In real-world databases, schemas alone are not enough to understand the data. Labels can be wrong, context can be missing, and business meaning is often undocumented. Without rich documentation, LLMs cannot reliably construct correct SQL queries.

    Datascribe solves this by creating **rich, structured, narrative documentation** that captures the real-world story of the data — its provenance, quirks, transformations, and limitations — in a way that future LLMs can cite to build correct queries.

    ## Key Principles
    - **The goal is to empower LLM-based text-to-SQL tools to perform professional-grade querying.**
    - **Narrative is critical**: Tell the story, not just the schema.
    - **Provenance is essential**: Where data came from, how it changed, what issues it has.
    - **Clarity prevents confusion**: If there is uncertainty or ambiguity, document it.
    - **Own the documentation**: Update it as you learn new information from the user or from querying the database.
    - **Table-by-table**: Work one dataset at a time to avoid overwhelming the user or yourself.

    ## File Structure
    Datascribe documentation must always be organized into a folder called `datascribe` (create it on the user's Desktop unless told otherwise). Inside it:

    datascribe/ 
    ├── _datasets.yml # gives a detailed overview of every dataset
    ├── <dataset_name>
    ├── ├── table_name>.yml # provides detailed narrative documentation about a single table including its columns
    └── ...


    ## YAML Structure for Dataset Files

    datascribe docs consist of distinct key types:
    - EMPIRICAL: information that can be derived from the data itself, such as table names, column names, and data types.
    - USERDEFINED: information that is defined by the user, such as descriptions, business context, and provenance.
    - LLMDEFINED: information that is generated by the LLM, such as summaries and clarifications.
    ```yaml
    # --_dataset.yml
    business_description: <USERDEFINED> <summary of the business and its data>
    path_to_service_account_file: <USERDEFINED> <path to the service account JSON file>
    project_id: <EMPIRICAL_or_USERDEFINED> <GCP Project ID>
    state_of_documentation: <LLMDEFINED> <summary of what is documented well and what needs clarification>
    data:
        table_name: <EMPIRICAL> <name of the table> 
        description: <USERDEFINED> <detailed description of the table’s purpose and business context> 
        provenance:
            created_by: <USERDEFINED> <who created the table>
            created_when: <EMPIRICAL> <approximate date or period>
            purpose: <USERDEFINED> <original business purpose>
            current_status: <USERDEFINED> <still in use, partially migrated, deprecated, etc>
            unique_identifier: <USERDEFINED or EMPIRICAL> <primary key column if known>
            migration_notes: <USERDEFINED> <details if data was migrated from or to this table> 
    ```
    ```yaml
    # --<dataset_name>/_<table_name>.yml
    table_path: <EMPIRICAL> <name of the table>
    columns:
        column_name: <EMPIRICAL> <column name> 
        data_type: <EMPIRICAL> <data type> 
        description: <USERDEFINED> <description of the column, including any known data quality issues>
        example_values: <EMPIRICAL> <examples of values in the column>
        missing_values: <EMPIRICAL> <percentage of missing values, if known>
        data_quality_issues: <EMPIRICAL and USERDEFINED> <known issues with the data, if any>
    ```




        Important Rules
        - If unsure, ask the user questions to clarify before documenting.
        - Don't invent relationships or common queries unless real evidence exists.
        - Mark deprecated datasets clearly if you document them.
        - Always summarize changes made to the documentation and why they were made.

        Behavior When Using Datascribe
        - Start by asking if the user has an existing datascribe folder and _datasets.yml file.
        - If not, create them and confirm the save location.
        - Work dataset-by-dataset:
            - Add the dataset to _datasets.yml (overview).
            - Create a new _<dataset_name>.yml for detailed documentation.
            - Confirm all USERDEFINED information explicitly with the user.
        - Remember: Datascribe's goal is to make future LLMs able to reason about and query messy data safely and intelligently.
        - Context is king. Provenance is essential. Questions are good. Clear documentation prevents future confusion.


    Because this documentation is so detailed it can sometimes be too much to write in one go. So pause after each table and ask the user if you should move on to the next one.
    """

@mcp.tool()
async def datascribe_data_querying_instructions() -> str:
    return """"
    When a user asks a question that requires querying data, rely heavily on the datascribe documentation.
    
    First, do not even begin to attempt the query unless the documentation's business_description field is filled out.
    If it is not, ask the user for a description of the organization that owns the data. 
    If you have the business_description, still do not make ANY assumptions about the data. 
    Under no circumstances should you query the data without first checking your SQL query with the user.
    Before querying the data, specify your assumptions to the user and ask if your querying approach is right.
    """

@mcp.tool()
async def run_bigquery_sql(sql: str, service_account_path: str, project_id: str ) -> str:
    """
    Run a SQL query on Google BigQuery using the provided service account credentials.
    Args:
        sql: A valid SQL query string to execute.
        service_account_path: Path to the service account JSON file.
        project_id: The GCP project ID to use for the BigQuery client.
    """

    credentials = service_account.Credentials.from_service_account_file(service_account_path)
    bq_client = bigquery.Client(credentials=credentials, project=project_id)

    loop = asyncio.get_event_loop()

    def query_bigquery():
        try:
            query_job = bq_client.query(sql)
            results = query_job.result()
            rows = [dict(row) for row in results]
            if not rows:
                return "Query executed successfully. No results returned."
            preview = rows[:10]
            return f"Query executed successfully. Showing up to 10 rows:\n{preview}"
        except Exception as e:
            return f"Error running query: {str(e)}"

    result = await loop.run_in_executor(None, query_bigquery)
    return result

if __name__ == "__main__":
    mcp.run(transport='stdio')
